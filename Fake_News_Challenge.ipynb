{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import models\n",
    "from sklearn.naive_bayes import MultinomialNB as MNB\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.metrics import classification_report\n",
    "#imports\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn import feature_extraction\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_wnl = nltk.WordNetLemmatizer()\n",
    "def normalize_word(w):\n",
    "    return _wnl.lemmatize(w).lower()\n",
    "def get_tokenized_lemmas(s):\n",
    "    return [normalize_word(t) for t in nltk.word_tokenize(s)]\n",
    "def clean(text):\n",
    "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric,removed https and @mentions too\n",
    "    text=re.sub(r'@[^ ]',' ',text)\n",
    "    text=re.sub(r'https?://[^ ]+',' ',text)\n",
    "    return \" \".join(re.findall(r'\\w+', text, flags=re.UNICODE)).lower()\n",
    "def remove_stopwords(l):\n",
    "    # Removes stopwords from a list of tokens\n",
    "    return [w for w in l if w not in feature_extraction.text.ENGLISH_STOP_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_wnl = nltk.WordNetLemmatizer()\n",
    "def normalize_word(w):\n",
    "    return _wnl.lemmatize(w).lower()\n",
    "def get_tokenized_lemmas(s):\n",
    "    return [normalize_word(t) for t in nltk.word_tokenize(s)]\n",
    "def clean(s):\n",
    "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
    "\n",
    "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()\n",
    "\n",
    "\n",
    "def remove_stopwords(l):\n",
    "    # Removes stopwords from a list of tokens\n",
    "    return [w for w in l if w not in feature_extraction.text.ENGLISH_STOP_WORDS]\n",
    "\n",
    "def gen_or_load_feats(feat_fn, headlines, bodies, feature_file):\n",
    "    if not os.path.isfile(feature_file):\n",
    "        feats = feat_fn(headlines, bodies)\n",
    "        np.save(feature_file, feats)\n",
    "\n",
    "    return np.load(feature_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def word_overlap_features(headlines, bodies):\n",
    "    X = []\n",
    "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
    "        clean_headline = clean(headline)\n",
    "        clean_body = clean(body)\n",
    "        clean_headline = get_tokenized_lemmas(clean_headline)\n",
    "        clean_body = get_tokenized_lemmas(clean_body)\n",
    "        features = [\n",
    "            len(set(clean_headline).intersection(clean_body)) / float(len(set(clean_headline).union(clean_body)))]\n",
    "        X.append(features)\n",
    "    return X\n",
    "\n",
    "\n",
    "def refuting_features(headlines, bodies):\n",
    "    _refuting_words = [\n",
    "        'fake',\n",
    "        'fraud',\n",
    "        'hoax',\n",
    "        'false',\n",
    "        'deny', 'denies',\n",
    "        # 'refute',\n",
    "        'not',\n",
    "        'despite',\n",
    "        'nope',\n",
    "        'doubt', 'doubts',\n",
    "        'bogus',\n",
    "        'debunk',\n",
    "        'pranks',\n",
    "        'retract'\n",
    "    ]\n",
    "    X = []\n",
    "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
    "        clean_headline = clean(headline)\n",
    "        clean_headline = get_tokenized_lemmas(clean_headline)\n",
    "        features = [1 if word in clean_headline else 0 for word in _refuting_words]\n",
    "        X.append(features)\n",
    "    return X\n",
    "\n",
    "\n",
    "def polarity_features(headlines, bodies):\n",
    "    _refuting_words = [\n",
    "        'fake',\n",
    "        'fraud',\n",
    "        'hoax',\n",
    "        'false',\n",
    "        'deny', 'denies',\n",
    "        'not',\n",
    "        'despite',\n",
    "        'nope',\n",
    "        'doubt', 'doubts',\n",
    "        'bogus',\n",
    "        'debunk',\n",
    "        'pranks',\n",
    "        'retract'\n",
    "    ]\n",
    "\n",
    "    def calculate_polarity(text):\n",
    "        tokens = get_tokenized_lemmas(text)\n",
    "        return sum([t in _refuting_words for t in tokens]) % 2\n",
    "    X = []\n",
    "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
    "        clean_headline = clean(headline)\n",
    "        clean_body = clean(body)\n",
    "        features = []\n",
    "        features.append(calculate_polarity(clean_headline))\n",
    "        features.append(calculate_polarity(clean_body))\n",
    "        X.append(features)\n",
    "    return np.array(X)\n",
    "\n",
    "\n",
    "def ngrams(input, n):\n",
    "    input = input.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(input) - n + 1):\n",
    "        output.append(input[i:i + n])\n",
    "    return output\n",
    "\n",
    "\n",
    "def chargrams(input, n):\n",
    "    output = []\n",
    "    for i in range(len(input) - n + 1):\n",
    "        output.append(input[i:i + n])\n",
    "    return output\n",
    "\n",
    "\n",
    "def append_chargrams(features, text_headline, text_body, size):\n",
    "    grams = [' '.join(x) for x in chargrams(\" \".join(remove_stopwords(text_headline.split())), size)]\n",
    "    grams_hits = 0\n",
    "    grams_early_hits = 0\n",
    "    grams_first_hits = 0\n",
    "    for gram in grams:\n",
    "        if gram in text_body:\n",
    "            grams_hits += 1\n",
    "        if gram in text_body[:255]:\n",
    "            grams_early_hits += 1\n",
    "        if gram in text_body[:100]:\n",
    "            grams_first_hits += 1\n",
    "    features.append(grams_hits)\n",
    "    features.append(grams_early_hits)\n",
    "    features.append(grams_first_hits)\n",
    "    return features\n",
    "\n",
    "\n",
    "def append_ngrams(features, text_headline, text_body, size):\n",
    "    grams = [' '.join(x) for x in ngrams(text_headline, size)]\n",
    "    grams_hits = 0\n",
    "    grams_early_hits = 0\n",
    "    for gram in grams:\n",
    "        if gram in text_body:\n",
    "            grams_hits += 1\n",
    "        if gram in text_body[:255]:\n",
    "            grams_early_hits += 1\n",
    "    features.append(grams_hits)\n",
    "    features.append(grams_early_hits)\n",
    "    return features\n",
    "\n",
    "\n",
    "def hand_features(headlines, bodies):\n",
    "\n",
    "    def binary_co_occurence(headline, body):\n",
    "        # Count how many times a token in the title\n",
    "        # appears in the body text.\n",
    "        bin_count = 0\n",
    "        bin_count_early = 0\n",
    "        for headline_token in clean(headline).split(\" \"):\n",
    "            if headline_token in clean(body):\n",
    "                bin_count += 1\n",
    "            if headline_token in clean(body)[:255]:\n",
    "                bin_count_early += 1\n",
    "        return [bin_count, bin_count_early]\n",
    "\n",
    "    def binary_co_occurence_stops(headline, body):\n",
    "        # Count how many times a token in the title\n",
    "        # appears in the body text. Stopwords in the title\n",
    "        # are ignored.\n",
    "        bin_count = 0\n",
    "        bin_count_early = 0\n",
    "        for headline_token in remove_stopwords(clean(headline).split(\" \")):\n",
    "            if headline_token in clean(body):\n",
    "                bin_count += 1\n",
    "                bin_count_early += 1\n",
    "        return [bin_count, bin_count_early]\n",
    "\n",
    "    def count_grams(headline, body):\n",
    "        # Count how many times an n-gram of the title\n",
    "        # appears in the entire body, and intro paragraph\n",
    "\n",
    "        clean_body = clean(body)\n",
    "        clean_headline = clean(headline)\n",
    "        features = []\n",
    "        features = append_chargrams(features, clean_headline, clean_body, 2)\n",
    "        features = append_chargrams(features, clean_headline, clean_body, 8)\n",
    "        features = append_chargrams(features, clean_headline, clean_body, 4)\n",
    "        features = append_chargrams(features, clean_headline, clean_body, 16)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 2)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 3)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 4)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 5)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 6)\n",
    "        return features\n",
    "\n",
    "    X = []\n",
    "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
    "        X.append(binary_co_occurence(headline, body)\n",
    "                 + binary_co_occurence_stops(headline, body)\n",
    "                 + count_grams(headline, body))\n",
    "\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merger(body,stance):\n",
    "    enter=[]\n",
    "    for i in stance['BodyID']:\n",
    "        enter.append(body['ArticleBody'][(body.BodyID[body.BodyID==i].index.tolist())[0]])\n",
    "    stance.insert(3,'ArticleBody',enter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stance_detect(train_stances,Test,model):\n",
    "    Labels=train_stances.drop(['Headline','BodyID','ArticleBody'],axis=1)\n",
    "    Label_Result=pd.DataFrame()\n",
    "    dat=hand_features(train_stances['Headline'],train_stances['ArticleBody'])\n",
    "    Label_Result['Stance_unrelated']=model.fit(dat,Labels['Stance_unrelated']).predict(hand_features(Test['Headline'],Test['ArticleBody']))\n",
    "    Label_Result['Stance_disagree']=model.fit(dat,Labels['Stance_disagree']).predict(hand_features(Test['Headline'],Test['ArticleBody']))\n",
    "    Label_Result['Stance_agree']=model.fit(dat,Labels['Stance_agree']).predict(hand_features(Test['Headline'],Test['ArticleBody']))\n",
    "    Label_Result['Stance_discuss']=model.fit(dat,Labels['Stance_discuss']).predict(hand_features(Test['Headline'],Test['ArticleBody']))\n",
    "    return Label_Result\n",
    "def Predict(train,test,Model=LR):\n",
    "    X=stance_detect(train,test,Model).stack()\n",
    "    def strip(text):\n",
    "        return text[7:]\n",
    "    return pd.Series(pd.Categorical(X[X!=0].index.get_level_values(1))).apply(strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns customized\n",
    "#enter train and test body\n",
    "train_bodies=pd.read_csv('train_bodies.csv')\n",
    "train_stances=pd.read_csv('train_stances.csv')\n",
    "train_bodies.columns=['BodyID','ArticleBody']\n",
    "train_stances.columns=['Headline','BodyID','Stance']\n",
    "train_stances=pd.get_dummies(train_stances,prefix=['Stance'],columns=['Stance'])\n",
    "merger(train_bodies,train_stances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39978it [02:10, 306.51it/s]\n",
      "9994it [00:32, 304.19it/s]\n",
      "9994it [00:32, 304.54it/s]\n",
      "9994it [00:33, 302.36it/s]\n",
      "9994it [00:33, 299.49it/s]\n"
     ]
    }
   ],
   "source": [
    "#Enter Train,Test and Model\n",
    "#Models-----------------------\n",
    "#LR  = LOGISTIC REGRESSION\n",
    "#MNB = MULTINOMIAL NAIVE BAYES\n",
    "L=len(train_stances)\n",
    "Submission=Predict(train_stances[round(0.2*L):],train_stances[:round(0.2*L)],LR())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       unrelated\n",
      "1         discuss\n",
      "2       unrelated\n",
      "3       unrelated\n",
      "4           agree\n",
      "5         discuss\n",
      "6       unrelated\n",
      "7       unrelated\n",
      "8       unrelated\n",
      "9       unrelated\n",
      "10      unrelated\n",
      "11        discuss\n",
      "12      unrelated\n",
      "13        discuss\n",
      "14      unrelated\n",
      "15      unrelated\n",
      "16      unrelated\n",
      "17        discuss\n",
      "18      unrelated\n",
      "19      unrelated\n",
      "20      unrelated\n",
      "21      unrelated\n",
      "22        discuss\n",
      "23      unrelated\n",
      "24      unrelated\n",
      "25      unrelated\n",
      "26      unrelated\n",
      "27          agree\n",
      "28        discuss\n",
      "29      unrelated\n",
      "          ...    \n",
      "9051    unrelated\n",
      "9052    unrelated\n",
      "9053    unrelated\n",
      "9054      discuss\n",
      "9055    unrelated\n",
      "9056    unrelated\n",
      "9057    unrelated\n",
      "9058    unrelated\n",
      "9059    unrelated\n",
      "9060    unrelated\n",
      "9061      discuss\n",
      "9062        agree\n",
      "9063      discuss\n",
      "9064    unrelated\n",
      "9065      discuss\n",
      "9066      discuss\n",
      "9067    unrelated\n",
      "9068    unrelated\n",
      "9069    unrelated\n",
      "9070      discuss\n",
      "9071    unrelated\n",
      "9072    unrelated\n",
      "9073        agree\n",
      "9074      discuss\n",
      "9075    unrelated\n",
      "9076    unrelated\n",
      "9077    unrelated\n",
      "9078      discuss\n",
      "9079      discuss\n",
      "9080      discuss\n",
      "Length: 9081, dtype: category\n",
      "Categories (4, object): [agree, disagree, discuss, unrelated]\n"
     ]
    }
   ],
   "source": [
    "print(Submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
